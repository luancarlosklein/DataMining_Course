{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeba5425",
   "metadata": {},
   "source": [
    "<h1> Desafio - Parte 2 </h1>\n",
    "<h3> By: Luan Carlos Klein </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec10b0",
   "metadata": {},
   "source": [
    "<h2> Importação das bibliotecas </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a434b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdf4be",
   "metadata": {},
   "source": [
    "<h2>Definições iniciais</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f027d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_path = os.path.join(\"data/reviewsTrainToronto.csv\")\n",
    "x_train = os.path.join(\"data/X_trainToronto.csv\")\n",
    "df_reviews = pd.read_csv(reviews_train_path)\n",
    "df = pd.read_csv(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b62a3b",
   "metadata": {},
   "source": [
    "<h2> Preparação dos dados </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89320bec",
   "metadata": {},
   "source": [
    "<b> Criação de Features </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4aebce",
   "metadata": {},
   "source": [
    "<h3> Feature 1: Sequencias Frequentes nas Categorias </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43b1f9",
   "metadata": {},
   "source": [
    "Encontra sequencias frequencias nas categorias (relacionadas a destaque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3629f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "## Função que recebe um dataframe e uma lista de regras relacionada as categorias. E adiciona nesse dataset, cada uma dessas regras\n",
    "## como True ou False, se o estabelecimento tiver ou não a sequencia\n",
    "def add_feature_categories(df, rules):\n",
    "    df_original = df.copy()\n",
    "    def check_attr(cats, components):\n",
    "        try:\n",
    "            for i in components:\n",
    "                if i not in cats:\n",
    "                    return False\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    for i in rules:\n",
    "        df_original[str(i)] = df_original['categories'].apply(lambda x: check_attr(x, i))\n",
    "    return df_original\n",
    "\n",
    "#######################################################################################################\n",
    "## Função que dado um dataframe, encontra quais são as as sequencias de categorais que implicam em ser ou não destaque\n",
    "def find_features_categories(df):\n",
    "    df_original = df\n",
    "    lista_categorias = df['categories']\n",
    "    lista_destaque = df['destaque']\n",
    "    transactions = []\n",
    "    cont = 0\n",
    "    for i in lista_categorias:\n",
    "        aux = str(i).split(', ')\n",
    "        if lista_destaque[cont] == 0:\n",
    "            aux.append(\"Não\")\n",
    "        else:\n",
    "            aux.append(\"Sim\")\n",
    "        transactions.append(aux)\n",
    "        cont += 1\n",
    "    ##Transforma as transações em um dataframe\n",
    "    te = TransactionEncoder()\n",
    "    encoded = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(encoded, columns=te.columns_)\n",
    "    #Calcula os conjuntos de itens (itemsets) com suporte >= 0,\n",
    "    frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
    "    #Regras com o valor de confiança mínimo escolhido\n",
    "    dfRegras= association_rules(frequent_itemsets, metric='confidence', min_threshold=0.65)\n",
    "    dfRegras = dfRegras[ (dfRegras['consequents'] == {\"Não\"}) | (dfRegras['consequents'] == {\"Sim\"})]\n",
    "    \n",
    "    itens = []\n",
    "    for i in dfRegras['antecedents']:\n",
    "        aux = list(i)\n",
    "        #if len(aux) == 1:\n",
    "        itens.append(aux[0])\n",
    "    return itens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254de16c",
   "metadata": {},
   "source": [
    "<h3> Feature 2: Sequencias Frequentes nos atributos </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b275a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com todos os atributos que há no dataset para o treino \n",
    "attr = ['BusinessParking_garage',\n",
    "        'BusinessParking_street', 'BusinessParking_validated',\n",
    "        'BusinessParking_lot', 'BusinessParking_valet', 'BikeParking',\n",
    "        'RestaurantsPriceRange2', 'ByAppointmentOnly', 'GoodForKids',\n",
    "        'WheelchairAccessible', 'RestaurantsReservations', 'RestaurantsAttire',\n",
    "        'WiFi', 'RestaurantsGoodForGroups', 'Alcohol', 'Caters',\n",
    "        'RestaurantsDelivery', 'NoiseLevel', 'OutdoorSeating',\n",
    "        'GoodForMeal_dessert', 'GoodForMeal_latenight', 'GoodForMeal_lunch',\n",
    "        'GoodForMeal_dinner', 'GoodForMeal_brunch', 'GoodForMeal_breakfast',\n",
    "        'RestaurantsTakeOut', 'HasTV', 'Ambience_romantic', 'Ambience_intimate',\n",
    "        'Ambience_classy', 'Ambience_hipster', 'Ambience_divey',\n",
    "        'Ambience_touristy', 'Ambience_trendy', 'Ambience_upscale',\n",
    "        'Ambience_casual', 'RestaurantsTableService', 'HappyHour',\n",
    "        'BestNights_monday', 'BestNights_tuesday', 'BestNights_friday',\n",
    "        'BestNights_wednesday', 'BestNights_thursday', 'BestNights_sunday',\n",
    "        'BestNights_saturday', 'Smoking', 'GoodForDancing', 'Music_dj',\n",
    "        'Music_background_music', 'Music_jukebox', 'Music_live', 'Music_video',\n",
    "        'Music_karaoke', 'Music_no_music', 'CoatCheck', 'DogsAllowed',\n",
    "        'DriveThru', 'AcceptsInsurance', 'BusinessAcceptsCreditCards',\n",
    "        'DietaryRestrictions_dairy-free', 'DietaryRestrictions_gluten-free',\n",
    "        'DietaryRestrictions_vegan', 'DietaryRestrictions_kosher',\n",
    "        'DietaryRestrictions_halal', 'DietaryRestrictions_soy-free',\n",
    "        'DietaryRestrictions_vegetarian', 'HairSpecializesIn_perms',\n",
    "        'HairSpecializesIn_coloring', 'HairSpecializesIn_extensions',\n",
    "        'HairSpecializesIn_curly', 'HairSpecializesIn_kids',\n",
    "        'HairSpecializesIn_straightperms', 'HairSpecializesIn_africanamerican',\n",
    "        'HairSpecializesIn_asian', 'BusinessAcceptsBitcoin', 'AgesAllowed', 'destaque']\n",
    "\n",
    "#######################################################################################################\n",
    "###### Função que dada o um datafrma com atributos e as regras, cria colunas de acordo com as regras\n",
    "def add_feature_attributes(df_attributes, rules):\n",
    "    def put_value(df, feature_name, components):\n",
    "        size = len(df.index)\n",
    "        df[feature_name] = True\n",
    "        cont = 0\n",
    "        while cont < size:\n",
    "            val = True\n",
    "            for i in components:\n",
    "                aux = i.split(\"?\")\n",
    "                if df.loc[cont, aux[0]] != aux[1]:\n",
    "                    val =  False\n",
    "                    break\n",
    "            df.loc[cont, feature_name] = val\n",
    "            cont += 1\n",
    "        return df.copy()\n",
    "\n",
    "    def add_attributes(df_features_in):\n",
    "        df_features = df_features_in\n",
    "        itens = []\n",
    "        for i in rules:\n",
    "            aux = list(i)\n",
    "            feature_name = str(aux)\n",
    "            df_features = put_value(df_features, feature_name, aux)\n",
    "            itens.append(feature_name)\n",
    "        return df_features\n",
    "    \n",
    "    df_prep = add_attributes(df_attributes)\n",
    "    attr_drop = attr[:len(attr)-1] ## Tira o destaque da lista de atibutos\n",
    "    df_prep =  df_prep.drop(attr_drop, axis=1)\n",
    "    return df_prep\n",
    "        \n",
    "########################################################################\n",
    "## Dado um dataframe, coloca todos os atributos como colunas\n",
    "def gerate_attributes_df(df_original):\n",
    "    df = df_original.copy()\n",
    "\n",
    "    ######################################################################\n",
    "    ## Função que separa os atributos em colunas\n",
    "    def separe_attributes(df):\n",
    "        df_x_train = df\n",
    "        ##### PARTE 1 - SEPARAÇÃO DE TODOS OS ATRIBUTOS PRESENTES NOS DADOS ############################\n",
    "        ## Pega a lista de todos os atributos\n",
    "        attrs = df_x_train['attributes']\n",
    "\n",
    "        ## Variaveis para armazenar as chaves e uma lista dos valores possiveis\n",
    "        values = {}\n",
    "\n",
    "        ## Percorre a lista dos atributos\n",
    "        for i in attrs:\n",
    "            ## Se for nan (nao tiver atributo), volta pro topo\n",
    "            if str(i) == \"nan\":\n",
    "                continue\n",
    "            else:\n",
    "                ## Transforma o objeto em um dicionário\n",
    "                aux = ast.literal_eval(i)\n",
    "\n",
    "                ## Percorre Esse dicionário\n",
    "                for j in aux:\n",
    "                    ## Transforma o valor (se der, vai transformar para um dicionário também)\n",
    "                    aux_2 = ast.literal_eval(aux[j])\n",
    "                    ## Se tiver valor nulo, apenas pula e volta para o inicio do loop\n",
    "                    if type(aux_2) == type(None):\n",
    "                        continue\n",
    "                    ## Verifica se a chave que está percorrendo já foi identificada\n",
    "                    if j in values:\n",
    "                        ## Se for um dicionário dentro\n",
    "                        if type(aux_2) is dict:\n",
    "                            ## Pecorre esse dicionário\n",
    "                            for k in aux_2:\n",
    "                                ## Verifica se a chave já foi identificada\n",
    "                                ## Senão foi, adiciona\n",
    "                                if k not in values[j]:\n",
    "                                    values[j][k] = \"\"\n",
    "                                    continue\n",
    "\n",
    "                    ############## Caso a chave não tenha sido identificada\n",
    "                    else:\n",
    "                        ## Verfica se é um dicionário ou um valor 'normal'\n",
    "                        if type(aux_2) is dict:\n",
    "                            ## Cria uma variavel para colocar internamente na armazenagem dos valores\n",
    "                            dict_interno = {}\n",
    "                            ## Percorre esse dicionário interno\n",
    "                            for k in aux_2:\n",
    "                                ## Preenche o dicionário interno\n",
    "                                dict_interno[k] = \"\"\n",
    "                            ## Faz o dicionário externo receber o dicionário interno criado\n",
    "                            values[str(j)] = dict_interno\n",
    "                        ## Caso seja um valor normal\n",
    "                        else:\n",
    "                            ## Adiciona nas variaveis, uma lista com aquele valor\n",
    "                            values[str(j)] = \"\"\n",
    "        ####################################################################\n",
    "\n",
    "        ##### PARTE 2 - CRIANDO COLUNAS PARA CADA ATRIBUTOS ###########\n",
    "        ## Se caso algum estabelecimento não tiver os dados, será colocado como FALSO por padrão\n",
    "\n",
    "        ## Função que recebe um dicionário (x), uma chave (i), e uma chave interna (k)\n",
    "        def get_value(x, i, k = False):\n",
    "            ## Se x for nulo, então retorna False\n",
    "            if str(x) == 'nan':\n",
    "                return False\n",
    "\n",
    "            ## Se não houver chave interna (ou seja, x[i] não é um dicionário)\n",
    "            if not k:\n",
    "                try:\n",
    "                    ## Transforma o X em um dicionario e retorna o valor de x[i]\n",
    "                    dic = ast.literal_eval(x)\n",
    "                    return dic[i]\n",
    "                except Exception as e:\n",
    "                    return False\n",
    "            ## Caso x[i] for um dicionário\n",
    "            else:\n",
    "                ## Tenta transforma e retorna o valor de x[i][k]\n",
    "                try:\n",
    "                    dic = ast.literal_eval(x)\n",
    "                    dic = ast.literal_eval(dic[i])\n",
    "                    return dic[k]\n",
    "                except Exception as e:\n",
    "                    return False\n",
    "\n",
    "        ## Percorre todas as chaves encontradas anteriormente        \n",
    "        for i in values.keys():\n",
    "            ## Verifica se os dados são de um dicionário\n",
    "            if type(values[i]) == dict:\n",
    "                ## Percorre o dicionário, se for o caso, e chama a função para retornar o valor\n",
    "                for k in values[i].keys():\n",
    "                     df_x_train[str(i) + \"_\" + str(k)] = df_x_train[\"attributes\"].apply(lambda x: get_value(x, i, k))\n",
    "            ## Cria a coluna com o atributo e chama a função para retornar o valor adequado\n",
    "            else:    \n",
    "                df_x_train[i] = df_x_train[\"attributes\"].apply(lambda x: get_value(x, i))\n",
    "        ############################### PARTE 3 - Garantir que todos os atributos foram colocados - Se não tiver, coloca tudo como falso ####\n",
    "        for i in attr:\n",
    "            if i not in df.columns:\n",
    "                df_x_train[i] = False\n",
    "\n",
    "        return df_x_train\n",
    "    \n",
    "    df_attributes = separe_attributes(df)\n",
    "    return df_attributes\n",
    "\n",
    "###########################################################################################################\n",
    "## Função que dado um dataframe, encontra quais são as as sequencias de atributos que implicam em ser ou não destaque\n",
    "def find_feature_attributes(df):\n",
    "    attrs = df['attributes']\n",
    "    lista_destaque = df['destaque']\n",
    "    transactions = []\n",
    "    cont = 0\n",
    "    for i in attrs:\n",
    "        trans_unit = []\n",
    "        try:\n",
    "            aux = ast.literal_eval(i)\n",
    "            for j in aux.keys():\n",
    "                aux2 = ast.literal_eval(aux[j])\n",
    "                if type(aux2) == dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    trans_unit.append(j+\"?\"+aux[j])#+\"_\"+aux[j]\n",
    "        except:\n",
    "            trans_unit = []\n",
    "        finally:\n",
    "            if lista_destaque[cont] == 0:\n",
    "                trans_unit.append(\"Não\")\n",
    "            else:\n",
    "                trans_unit.append(\"Sim\")\n",
    "            transactions.append(trans_unit)\n",
    "        cont += 1\n",
    "\n",
    "    ##Transforma as transações em um dataframe\n",
    "    te = TransactionEncoder()\n",
    "    encoded = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(encoded, columns=te.columns_)\n",
    "    #Calcula os conjuntos de itens (itemsets) com suporte >= 0,\n",
    "    frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "    #Regras com o valor de confiança mínimo escolhido\n",
    "    dfRegras= association_rules(frequent_itemsets, metric='confidence', min_threshold=0.85)\n",
    "    dfRegras = dfRegras[ (dfRegras['consequents'] == {\"Não\"}) | (dfRegras['consequents'] == {\"Sim\"})]\n",
    "    \n",
    "    return dfRegras['antecedents'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce8c49",
   "metadata": {},
   "source": [
    "<h3> Feature 3 - Sentimentos das reviews </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc53345",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "## Função que adiciona uma feature sobre o humor dos reviews. Faz a média das reviews sobre cada estabelecimento\n",
    "def add_feature_sentiment_reviews(df_reviews, df):\n",
    "    sent = SentimentIntensityAnalyzer()\n",
    "    df_reviews_relevant = df_reviews.copy()\n",
    "    df_reviews_relevant['sentiment'] = df_reviews_relevant['text'].apply(lambda x: sent.polarity_scores(x)['compound'])\n",
    "    df_reviews_relevant = df_reviews_relevant.groupby(['business_id']).mean().reset_index()\n",
    "    df_reviews_relevant = df_reviews_relevant[['business_id', 'sentiment']]\n",
    "    df2 = df.merge(df_reviews_relevant, left_on='business_id', right_on='business_id', how='left')\n",
    "    df2['sentiment'] = df2['sentiment'].fillna(0)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4c312",
   "metadata": {},
   "source": [
    "\n",
    "<h3> Feature 4 - Proximidade de Lugares Destaques - Não destaques </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc0afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "## Função que cria dois modelos de clusterização com o DBSCAN, um para destaques e outro para não destaques, \n",
    "## e faz a classificação do dateset de acordo com a localização\n",
    "def add_features_places_train(df):\n",
    "    df_use = df.copy()\n",
    "    ids = df_use['business_id']\n",
    "    \n",
    "    df_pop = df_use[df_use['destaque'] == 1]\n",
    "    ids_pop = df_pop['business_id']\n",
    "    df_no_pop = df_use[df_use['destaque'] == 0]\n",
    "    ids_no_pop = df_pop['business_id']\n",
    "\n",
    "    df_pop = df_pop[['latitude', 'longitude']]\n",
    "    df_no_pop = df_no_pop[['latitude', 'longitude']]\n",
    "    \n",
    "    dbscanConfig_pop = DBSCAN(eps=0.001, min_samples=40)\n",
    "    dbscanConfig_no_pop = DBSCAN(eps=0.0008, min_samples=50)\n",
    "    \n",
    "    dbscanResults_pop = dbscanConfig_pop.fit(df_pop)\n",
    "    dbscanResults_no_pop = dbscanConfig_no_pop.fit(df_no_pop)\n",
    "    \n",
    "    list_pop = list(zip(ids_pop, dbscanResults_pop.labels_))\n",
    "    df_pop_result = pd.DataFrame(list_pop, columns=['business_id', 'pop_zone'])\n",
    "    \n",
    "    list_no_pop = list(zip(ids_no_pop, dbscanResults_no_pop.labels_))\n",
    "    df_no_pop_result = pd.DataFrame(list_pop, columns=['business_id', 'no_pop_zone'])\n",
    "    \n",
    "    df = df.merge(df_pop_result, left_on='business_id', right_on='business_id', how='left')\n",
    "    df = df.merge(df_no_pop_result, left_on='business_id', right_on='business_id', how='left')\n",
    "    \n",
    "    df['pop_zone'] = df['pop_zone'].fillna(-1)\n",
    "    df['no_pop_zone'] = df['no_pop_zone'].fillna(-1)\n",
    "    \n",
    "    return df, dbscanResults_pop, dbscanResults_no_pop\n",
    "\n",
    "###########################################################################################################\n",
    "## Função que dado um novo dataframe, classifica a localização de acordo com os modelos treinados\n",
    "def add_features_places_test(df, model_pop, model_no_pop):\n",
    "    df_use = df.copy()\n",
    "    ids = df_use['business_id']\n",
    "    df_use = df_use[['latitude', 'longitude']]\n",
    "    ## Fonte: https://stackoverflow.com/questions/27822752/scikit-learn-predicting-new-points-with-dbscan\n",
    "    def dbscan_predict(model, X):\n",
    "        nr_samples = X.shape[0]\n",
    "        y_new = np.ones(shape=nr_samples, dtype=int) * -1\n",
    "        for i in range(nr_samples):\n",
    "            diff = model.components_ - list(X.iloc[i, :])\n",
    "            dist = np.linalg.norm(diff, axis=1)\n",
    "            shortest_dist_idx = np.argmin(dist)\n",
    "            if dist[shortest_dist_idx] < model.eps:\n",
    "                y_new[i] = model.labels_[model.core_sample_indices_[shortest_dist_idx]]\n",
    "        return y_new\n",
    "    pred_zone_pop = dbscan_predict(model_pop, df_use)\n",
    "    pred_zone_no_pop = dbscan_predict(model_no_pop, df_use)\n",
    "    df['pop_zone'] = pred_zone_pop\n",
    "    df['no_pop_zone'] = pred_zone_no_pop\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb0380",
   "metadata": {},
   "source": [
    "<h3> Preparação dos dados para os modelos </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2980382",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chama os métodos que irão definir os atributos\n",
    "features_categories = find_features_categories(df)\n",
    "features_attr = find_feature_attributes(df)\n",
    "dbscanResults_pop = None\n",
    "dbscanResults_no_pop = None\n",
    "\n",
    "###########################################################################################################\n",
    "## Função que recebe os dataframes e realiza o pré-processamento dos dados, tanto para treino como para teste\n",
    "## retorna um dataframe pronto para ser usado nos modelos\n",
    "def pre_processor(df, df_reviews, train=True):\n",
    "    df_use = df.copy()\n",
    "    print(\"Adding Features - Attributes\")\n",
    "    df_attributes = gerate_attributes_df(df_use)\n",
    "    df_use = add_feature_attributes(df_attributes, features_attr)\n",
    "    print(\"Adding Features - Categories\")\n",
    "    df_use = add_feature_categories(df_use, features_categories)\n",
    "    print(\"Adding Features - Sentiment\")\n",
    "    df_use = add_feature_sentiment_reviews(df_reviews, df_use)\n",
    "    print(\"Adding Features - Pop Zone\")\n",
    "    if train == True:\n",
    "        global dbscanResults_pop\n",
    "        global dbscanResults_no_pop\n",
    "        df_use, dbscanResults_pop, dbscanResults_no_pop = add_features_places_train(df_use)\n",
    "    else:\n",
    "        df_use = add_features_places_test(df_use, dbscanResults_pop, dbscanResults_no_pop)\n",
    "    print(\"Scaller Latitude and Longitude\")\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    df_use['latitude'] = scaler.fit_transform(df[['latitude']])\n",
    "    df_use['longitude'] = scaler.fit_transform(df[['longitude']])\n",
    "    coluns_drop = ['business_id', 'name', 'address', 'postal_code', 'review_count', 'attributes', 'is_open', 'hours', 'loc', 'categories']\n",
    "    df_dropped = df_use.drop(coluns_drop, axis=1)\n",
    "    for i in df_dropped.columns:\n",
    "        if i not in ['latitude', 'longitude', 'sentiment', 'pop_zone', 'no_pop_zone']:\n",
    "            df_dropped[i] = pd.Categorical(df_dropped[i])\n",
    "            df_dropped[i] = df_dropped[i].cat.codes\n",
    "    return df_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784de42",
   "metadata": {},
   "source": [
    "<h2> Chama a função de preparação dos dados e divisão em dados de treino e teste</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db157867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Features - Attributes\n",
      "Adding Features - Categories\n",
      "Adding Features - Sentiment\n",
      "Adding Features - Pop Zone\n",
      "Scaller Latitude and Longitude\n"
     ]
    }
   ],
   "source": [
    "df_process = pre_processor(df, df_reviews)\n",
    "df_train, df_teste = train_test_split(df_process, test_size=0.5, random_state=42)\n",
    "X = df_train.drop('destaque', axis=1)\n",
    "y = df_train['destaque']\n",
    "X_test = df_teste.drop('destaque', axis=1)\n",
    "y_test = df_teste['destaque']\n",
    "columns_train = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce1110",
   "metadata": {},
   "source": [
    "<h4> Função para a realização do arredondamento e testes de acertos </h4>\n",
    "É necessária porque a regressão linear e a rede neural classificam não como 1 ou 0, então criamos uma função para arredondar,\n",
    "e depois uma função para comparar acertos e erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31bde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "## Função que recebe uma lista, e arredonda os valores dela\n",
    "def round_list(array):\n",
    "    new_array = []\n",
    "    for i in array:\n",
    "        new_array.append(round(i))\n",
    "    return new_array\n",
    "\n",
    "###########################################################################################################\n",
    "## Função que recebe os resultados de um modelo e os dados reais, e realiza a comparação, retornando a porcentagem de acerto\n",
    "def round_compare(test, real):\n",
    "    test_round = []\n",
    "    for i in test:\n",
    "        test_round.append(round(i))\n",
    "    correct = 0\n",
    "    cont = 0\n",
    "    real_list = list(real)\n",
    "    while cont < len(real_list):\n",
    "        if test_round[cont] == real_list[cont]:\n",
    "            correct += 1\n",
    "        cont += 1\n",
    "    return correct/len(real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30a0ff",
   "metadata": {},
   "source": [
    "<h2> Realiza o treinamento em uma árvore de descisão </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d53883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8004978220286247"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instância da árvore de decisão\n",
    "clf_random_forest = RandomForestClassifier(max_depth = 10, random_state=42)\n",
    "# Faz o treinamento com os dados de interesse\n",
    "clf_random_forest.fit(X, y)\n",
    "result_trees = clf_random_forest.predict(X_test)\n",
    "clf_random_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f2d8d",
   "metadata": {},
   "source": [
    "<h2> Realiza o treinamento em uma regressão linear </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f220d2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7824517734909769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X,y)\n",
    "result_regression = linear_regression.predict(X_test)\n",
    "round_compare(result_regression, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aba00",
   "metadata": {},
   "source": [
    "<h2> Realiza o treinamento em uma rede neural </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c487d044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7970130678282514"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(15, 5), random_state=1, max_iter=5000, activation='logistic')\n",
    "mlp_regressor = mlp.fit(X, y)\n",
    "result_neural = mlp_regressor.predict(X_test)\n",
    "round_compare(result_neural, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56664de",
   "metadata": {},
   "source": [
    "<h2> Função que realiza o consenso entre os três algoritmos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cba2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consense_function(r1, r2, r3):\n",
    "    r1_list = list(r1)\n",
    "    r2_list = list(r2)\n",
    "    r3_list = list(r3)\n",
    "    final_dec = []\n",
    "    cont = 0\n",
    "    while cont < len(r1):\n",
    "        total_votes = r1_list[cont] + r2_list[cont] + r3_list[cont]\n",
    "        if total_votes > 1:\n",
    "            final_dec.append(1)\n",
    "        else:\n",
    "            final_dec.append(0)\n",
    "        cont += 1\n",
    "    return final_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a42df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7991288114499067"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consense_result = consense_function(result_trees, result_regression, result_neural)\n",
    "round_compare(consense_result, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044333cf",
   "metadata": {},
   "source": [
    "<h3> Criando o Modelo para ser aplicado na prática </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c055e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df, df_reviews):\n",
    "    ids = df['business_id']\n",
    "    data = pre_processor(df, df_reviews, train=False)\n",
    "    \n",
    "    not_in_model = []\n",
    "    for i in data.columns:\n",
    "        if i not in columns_train:\n",
    "            not_in_model.append(i)\n",
    "    data =  data.drop(not_in_model, axis=1)\n",
    "    \n",
    "    result_trees = clf_random_forest.predict(data)\n",
    "\n",
    "    result_regression = round_list(linear_regression.predict(data))\n",
    "    result_neural = round_list(mlp_regressor.predict(data))\n",
    "    consense_result = consense_function(result_trees, result_regression, result_neural)\n",
    "    list_tuple = list(zip(ids, consense_result))\n",
    "    df_final = pd.DataFrame(list_tuple, columns=['business_id', 'destaque'])\n",
    "    df_final.to_csv(\"output.txt\", header=True, index=False)\n",
    "    print(\"The process is finished sucessfully!\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c90c5",
   "metadata": {},
   "source": [
    "<h1> Colocando o Modelo treinado em prática </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a13b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_path = os.path.join(\"data/reviewsTestToronto.csv\")\n",
    "x_test = os.path.join(\"data/X_testToronto.csv\")\n",
    "df_reviews_test = pd.read_csv(reviews_test_path)\n",
    "df_test = pd.read_csv(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a451fb",
   "metadata": {},
   "source": [
    "<h2> Fazendo a Predição </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe5c7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Features - Attributes\n",
      "Adding Features - Categories\n",
      "Adding Features - Sentiment\n",
      "Adding Features - Pop Zone\n",
      "Scaller Latitude and Longitude\n",
      "The process is finished sucessfully!\n"
     ]
    }
   ],
   "source": [
    "df_final = model(df_test, df_reviews_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
